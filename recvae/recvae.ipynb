{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import bottleneck as bn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='RecVAE for Sequential Recommendation')\n",
    "\n",
    "parser.add_argument('--data_dir', type=str, default='../data/train/', help='Movielens train dataset path')\n",
    "parser.add_argument('--min_items_per_user', type=int, default=5)\n",
    "parser.add_argument('--min_users_per_item', type=int, default=0)\n",
    "parser.add_argument('--heldout_users', type=int, default=3000)\n",
    "parser.add_argument('--seed', type=int, default=42, help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true', help='use CUDA')\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=500)\n",
    "parser.add_argument('--latent_dim', type=int, default=200)\n",
    "parser.add_argument('--batch_size', type=int, default=500)\n",
    "parser.add_argument('--beta', type=float, default=0.1)\n",
    "parser.add_argument('--gamma', type=float, default=0.005)\n",
    "parser.add_argument('--lr', type=float, default=2e-4)\n",
    "parser.add_argument('--weight_decay', type=float, default=0)\n",
    "parser.add_argument('--scheduler', type=str, default='None') \n",
    "parser.add_argument('--dropout_rate', type=float, default=0.5)\n",
    "parser.add_argument('--n_epochs', type=int, default=150)\n",
    "parser.add_argument('--n_enc_epochs', type=int, default=3)\n",
    "parser.add_argument('--n_dec_epochs', type=int, default=1)\n",
    "parser.add_argument('--not_alternating', type=bool, default=False)\n",
    "\n",
    "exp_idx = 5\n",
    "parser.add_argument('--save', type=str, default=f'./ckpts/model_exp{exp_idx}.pt',\n",
    "                    help='path to save the final model')\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwatchstep\u001b[0m (\u001b[33mwatchstep_\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login to WandB\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwatchstep\u001b[0m (\u001b[33mnew-recs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/ml/input/recvae/wandb/run-20230612_013322-4kwlz8ov</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/new-recs/movierec/runs/4kwlz8ov' target=\"_blank\">usual-sea-74</a></strong> to <a href='https://wandb.ai/new-recs/movierec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/new-recs/movierec' target=\"_blank\">https://wandb.ai/new-recs/movierec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/new-recs/movierec/runs/4kwlz8ov' target=\"_blank\">https://wandb.ai/new-recs/movierec/runs/4kwlz8ov</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecVAE_watchstep_exp5\n"
     ]
    }
   ],
   "source": [
    "# initialize WandB\n",
    "run = wandb.init(entity=\"new-recs\", project=\"movierec\", tags=[\"RecVAE\"], group=\"RecVAE_watchstep\", config=args)\n",
    "wandb.run.name = f\"RecVAE_watchstep_exp{exp_idx}\"\n",
    "run.save()\n",
    "print(wandb.run.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `heldout_users` : select 3000 users as heldout user, 3000 users as validation users, and the rest of the users for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed(args.seed)\n",
    "  args.cuda = True\n",
    "  \n",
    "device = torch.device('cuda:0' if args.cuda else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = args.data_dir\n",
    "min_uc = args.min_items_per_user\n",
    "min_sc = args.min_users_per_item\n",
    "n_heldout_users = args.heldout_users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dawenl/vae_cf\n",
    "# https://github.com/ilya-shenbin/RecVAE \n",
    "\n",
    "# Pandas version : 2.0.2\n",
    "def get_count(tp, id):\n",
    "  playcount_groupbyid = tp[[id]].groupby(id)\n",
    "  count = playcount_groupbyid.size()\n",
    "   \n",
    "  return count\n",
    "\n",
    "# Only keep items that are clicked on by at least 5 users \n",
    "def filter_triplets(tp, min_uc=min_uc, min_sc=min_sc):\n",
    "  # Only keep the triplets for items which were clicked on by at least min_sc users.\n",
    "  if min_sc > 0:\n",
    "    item_count = get_count(tp, 'item')\n",
    "    tp = tp[tp['item'].isin(item_count.index[item_count >= min_sc])]\n",
    "    \n",
    "  # Only keep the triplets for users who clicked on at least min_uc items\n",
    "  # After, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "  if min_uc > 0:\n",
    "    user_count = get_count(tp, 'user')\n",
    "    tp = tp[tp['user'].isin(user_count.index[user_count >= min_uc])]\n",
    "  \n",
    "  \n",
    "  # Update both usercount and itemcount after filtering\n",
    "  user_count, item_count = get_count(tp, 'user'), get_count(tp, 'item')\n",
    "  return tp, user_count, item_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>4643</td>\n",
       "      <td>1230782529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>170</td>\n",
       "      <td>1230782534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>531</td>\n",
       "      <td>1230782539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>616</td>\n",
       "      <td>1230782542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>2140</td>\n",
       "      <td>1230782563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item        time\n",
       "0    11  4643  1230782529\n",
       "1    11   170  1230782534\n",
       "2    11   531  1230782539\n",
       "3    11   616  1230782542\n",
       "4    11  2140  1230782563"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(os.path.join(data_dir, 'train_ratings.csv'), header=0)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering\n",
      "============================================\n",
      "5154471 events from 31360 users & 6807 movies\n",
      "sparsity: 2.415%\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print('After filtering')\n",
    "print('============================================')\n",
    "print(f'{raw_data.shape[0]} events from {user_activity.shape[0]} users & {item_popularity.shape[0]} movies')\n",
    "print(f'sparsity: {sparsity*100:.3f}%')\n",
    "\n",
    "# 원본 데이터셋과 똑같은 결과 (이미 5번 이상의 리뷰가 있는 user들로만 이루어진 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user\n",
       "11    376\n",
       "14    180\n",
       "18     77\n",
       "25     91\n",
       "31    154\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_activity.head() # user별 리뷰수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item\n",
       "1    12217\n",
       "2     3364\n",
       "3      734\n",
       "4       43\n",
       "5      590\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_popularity.head() # item별 리뷰수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([    11,     14,     18,     25,     31,     35,     43,     50,     58,\n",
       "           60,\n",
       "       ...\n",
       "       138459, 138461, 138470, 138471, 138472, 138473, 138475, 138486, 138492,\n",
       "       138493],\n",
       "      dtype='int64', name='user', length=31360)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_uid = user_activity.index\n",
    "unique_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before shuffling\n",
      "================\n",
      "Index([    11,     14,     18,     25,     31,     35,     43,     50,     58,\n",
      "           60,\n",
      "       ...\n",
      "       138459, 138461, 138470, 138471, 138472, 138473, 138475, 138486, 138492,\n",
      "       138493],\n",
      "      dtype='int64', name='user', length=31360)\n",
      "\n",
      "After shuffling\n",
      "================\n",
      "Index([ 81259,  11986,  67552, 127325, 115853,   5192,  93410, 124003,    422,\n",
      "       115657,\n",
      "       ...\n",
      "        27377,  96934,  49486,  52439,  94582, 131351,  23478,   3671,  69383,\n",
      "       103755],\n",
      "      dtype='int64', name='user', length=31360)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle User Indices\n",
    "unique_uid = user_activity.index\n",
    "\n",
    "unique_uid_before_shuffling = user_activity.index\n",
    "\n",
    "print('Before shuffling')\n",
    "print('================')\n",
    "print(unique_uid)\n",
    "\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]\n",
    "print('\\nAfter shuffling')\n",
    "print('================')\n",
    "print(unique_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of all users: 31360\n",
      "# of train users: 25360\n",
      "# of validation users: 3000\n",
      "# of test users: 3000\n"
     ]
    }
   ],
   "source": [
    "# train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "n_heldout_users = args.heldout_users # 10000\n",
    "\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users*2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users*2):(n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]\n",
    "\n",
    "print(f'# of all users: {n_users}')\n",
    "print(f'# of train users: {len(tr_users)}')\n",
    "print(f'# of validation users: {len(vd_users)}')\n",
    "print(f'# of test users: {len(te_users)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "  data_grouped_by_user = data.groupby('user')\n",
    "  tr_list, te_list = [], []\n",
    "  \n",
    "  np.random.seed(args.seed)\n",
    "  \n",
    "  for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "    n_items_u = len(group)\n",
    "    \n",
    "    if n_items_u >= 5:\n",
    "      idx = np.zeros(n_items_u, dtype='bool')\n",
    "      idx[np.random.choice(n_items_u, size=int(test_prop*n_items_u), replace=False).astype('int64')] = True\n",
    "      \n",
    "      tr_list.append(group[np.logical_not(idx)])\n",
    "      te_list.append(group[idx])\n",
    "      \n",
    "    else:\n",
    "      tr_list.append(group)\n",
    "      \n",
    "    if i % 1000 == 0:\n",
    "      print(f'{i} users sampled')\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "  data_tr = pd.concat(tr_list)\n",
    "  data_te = pd.concat(te_list)\n",
    "  \n",
    "  return data_tr, data_te\n",
    "\n",
    "def numerize(tp, profile2id, show2id):\n",
    "  uid = tp['user'].apply(lambda x: profile2id[x])\n",
    "  sid = tp['item'].apply(lambda x: show2id[x])\n",
    "  \n",
    "  return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data는 전체 데이터 모두 사용\n",
    "train_plays = raw_data.loc[raw_data['user'].isin(tr_users)]\n",
    "\n",
    "unique_sid = pd.unique(train_plays['item'])\n",
    "\n",
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir):\n",
    "  os.makedirs(data_dir)\n",
    "\n",
    "with open(os.path.join(data_dir, 'unique_sid.txt'), 'w') as f:\n",
    "  for sid in unique_sid:\n",
    "    f.write(f'{sid}\\n')\n",
    "    \n",
    "with open(os.path.join(data_dir, 'unique_uid.txt'), 'w') as f:\n",
    "  for uid in unique_uid:\n",
    "    f.write(f'{uid}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n"
     ]
    }
   ],
   "source": [
    "# validation / test data는 input인 tr 데이터와 정답 확인하기 위한 te 데이터로 분리\n",
    "vad_plays = raw_data.loc[raw_data['user'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['item'].isin(unique_sid)]\n",
    "\n",
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
    "\n",
    "test_plays = raw_data.loc[raw_data['user'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['item'].isin(unique_sid)]\n",
    "\n",
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data into (user_index, item_index) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numerize(train_plays, profile2id, show2id)\n",
    "# train_data.to_csv(os.path.join(data_dir, 'train.csv'), index=False)\n",
    "\n",
    "vad_data_tr = numerize(vad_plays_tr, profile2id, show2id)\n",
    "# vad_data_tr.to_csv(os.path.join(data_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "vad_data_te = numerize(vad_plays_te, profile2id, show2id)\n",
    "# vad_data_te.to_csv(os.path.join(data_dir, 'validation_te.csv'), index=False)\n",
    "\n",
    "test_data_tr = numerize(test_plays_tr, profile2id, show2id)\n",
    "# test_data_tr.to_csv(os.path.join(data_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "test_data_te = numerize(test_plays_te, profile2id, show2id)\n",
    "# test_data_te.to_csv(os.path.join(data_dir, 'test_te.csv'), index=False)\n",
    "\n",
    "# # uid sorting for submission (before shuffling )\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid_before_shuffling))\n",
    "inference_data = numerize(raw_data, profile2id, show2id)\n",
    "# inference_data.to_csv(os.path.join(data_dir, 'inference.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13266</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uid  sid\n",
       "0  13266    0\n",
       "1  13266    1\n",
       "2  13266    2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>28125</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>28125</td>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>28125</td>\n",
       "      <td>1418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   sid\n",
       "2133  28125   887\n",
       "2134  28125  1162\n",
       "2135  28125  1418"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vad_data_tr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>30380</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>30380</td>\n",
       "      <td>1717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>30380</td>\n",
       "      <td>2449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   sid\n",
       "1514  30380   468\n",
       "1515  30380  1717\n",
       "1516  30380  2449"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid  sid\n",
       "0    0    0\n",
       "1    0    1\n",
       "2    0    2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_data.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "  '''\n",
    "  Load Movielens dataset for RecVAE\n",
    "  ''' \n",
    "  def __init__(self, data_dir):\n",
    "    self.data_dir = data_dir\n",
    "    assert os.path.exists(data_dir), \"DATA NOT EXIT\"\n",
    "    \n",
    "    self.n_items = self.load_n_items()\n",
    "  \n",
    "  def load_data(self, data_type='train'):\n",
    "    if data_type == 'train':\n",
    "      return self._load_train_data(data_type)\n",
    "    elif data_type == 'validation':\n",
    "      return self._load_tr_te_data(data_type)\n",
    "    elif data_type == 'test':\n",
    "      return self._load_tr_te_data(data_type)\n",
    "    elif data_type == 'inference':\n",
    "      return self._load_train_data(data_type)\n",
    "    else:\n",
    "      raise ValueError('datatype should be in [train, validation, test, inference]')\n",
    "  \n",
    "  def load_items(self):\n",
    "    unique_sid = np.loadtxt(os.path.join(self.data_dir, 'unique_sid.txt'))\n",
    "    return unique_sid\n",
    "  \n",
    "  def load_n_items(self):\n",
    "    unique_sid = []\n",
    "    with open(os.path.join(self.data_dir, 'unique_sid.txt'), 'r') as f:\n",
    "      for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "        \n",
    "    n_items = len(unique_sid)\n",
    "    return n_items\n",
    "  \n",
    "  def load_n_users(self):\n",
    "    unique_uid = []\n",
    "    with open(os.path.join(self.data_dir, 'unique_uid.txt'), 'r') as f:\n",
    "      for line in f:\n",
    "        unique_uid.append(line.strip())\n",
    "    \n",
    "    n_users = len(unique_uid)\n",
    "    return n_users\n",
    "  \n",
    "  def _load_train_data(self, data_type='train'):\n",
    "    path = os.path.join(self.data_dir, '{}.csv'.format(data_type))\n",
    "    \n",
    "    tp = pd.read_csv(path)\n",
    "    n_users = tp['uid'].max() + 1\n",
    "    \n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, self.n_items))\n",
    "    \n",
    "    return data\n",
    "  \n",
    "  def _load_tr_te_data(self, data_type='test'):\n",
    "    tr_path = os.path.join(self.data_dir, '{}_tr.csv'.format(data_type))\n",
    "    te_path = os.path.join(self.data_dir, '{}_te.csv'.format(data_type))\n",
    "\n",
    "    tp_tr = pd.read_csv(tr_path)\n",
    "    tp_te = pd.read_csv(te_path)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                                (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                                (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://www.researchgate.net/profile/Elena-Tutubalina/publication/338158079/figure/fig1/AS:839837569540096@1577244288014/RecVAE-architecture.ppm\" width='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "  return x.mul(torch.sigmoid(x))\n",
    "\n",
    "def log_norm_pdf(x, mu, logvar):\n",
    "  return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
    "\n",
    "class CompositePrior(nn.Module):\n",
    "  def __init__(self, hidden_dim, latent_dim, input_dim, mixture_weights=[3/20, 3/4, 1/10]):\n",
    "    super(CompositePrior, self).__init__()\n",
    "    \n",
    "    self.mixture_weights = mixture_weights\n",
    "    \n",
    "    self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "    self.mu_prior.data.fill_(0)\n",
    "    \n",
    "    self.logvar_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "    self.logvar_prior.data.fill_(0)\n",
    "    \n",
    "    self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "    self.logvar_uniform_prior.data.fill_(10)\n",
    "    \n",
    "    self.encoder_old = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "    self.encoder_old.requires_grad_(False)\n",
    "    \n",
    "  def forward(self, x, z):\n",
    "    post_mu, post_logvar = self.encoder_old(x, 0)\n",
    "    \n",
    "    # encoder의 output과 이전 epoch의 파라미터를 지정한 encoder (as a pior)간의 KL Divergence\n",
    "    stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
    "    post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
    "    unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_uniform_prior)\n",
    "    \n",
    "    gaussians = [stnd_prior, post_prior, unif_prior]\n",
    "    gaussians = [g.add(np.log(w)) for g, w in zip(gaussians, self.mixture_weights)]\n",
    "    \n",
    "    density_per_gaussian = torch.stack(gaussians, dim=-1)\n",
    "    \n",
    "    return torch.logsumexp(density_per_gaussian, dim=-1)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
    "    super(Encoder, self).__init__()\n",
    "    \n",
    "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "    self.ln1 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.ln2 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "    self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.ln3 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "    self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.ln4 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "    self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.ln5 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "    self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "    self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "    \n",
    "  def forward(self, x, dropout_rate):\n",
    "    norm = x.pow(2).sum(dim=-1).sqrt()\n",
    "    x = x / norm[:, None]\n",
    "    \n",
    "    x = F.dropout(x, p=dropout_rate, training=self.training)\n",
    "    \n",
    "    h1 = self.ln1(swish(self.fc1(x)))\n",
    "    h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
    "    h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
    "    h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
    "    h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
    "    \n",
    "    return self.fc_mu(h5), self.fc_logvar(h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecVAE(nn.Module):\n",
    "  def __init__(self, hidden_dim, latent_dim, input_dim):\n",
    "    super(RecVAE, self).__init__()\n",
    "    \n",
    "    self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "    self.prior = CompositePrior(hidden_dim, latent_dim, input_dim)\n",
    "    self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "    \n",
    "  def reparameterize(self, mu, logvar):\n",
    "    if self.training:\n",
    "      std = torch.exp(0.5*logvar)\n",
    "      eps = torch.randn_like(std)\n",
    "      return eps.mul(std).add_(mu)\n",
    "    else:\n",
    "      return mu\n",
    "  \n",
    "  def forward(self, user_ratings, beta=None, gamma=1, dropout_rate=0.5, calculate_loss=True):\n",
    "    mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate)\n",
    "    z = self.reparameterize(mu, logvar)\n",
    "    x_pred = self.decoder(z)\n",
    "    \n",
    "    if calculate_loss:\n",
    "      if gamma:\n",
    "        norm = user_ratings.sum(dim=-1)\n",
    "        kl_weight = gamma*norm\n",
    "      elif beta:\n",
    "        kl_weight = beta\n",
    "      \n",
    "      mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
    "      kld = (log_norm_pdf(z, mu, logvar) - self.prior(user_ratings, z)).sum(dim=-1).mul(kl_weight).mean()\n",
    "      negative_elbo = -(mll - kld)\n",
    "      \n",
    "      return (mll, kld), negative_elbo\n",
    "    \n",
    "    else:\n",
    "      return x_pred\n",
    "  \n",
    "  def update_prior(self):\n",
    "    self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(X_pred, heldout_batch, k=100):\n",
    "  '''\n",
    "  normalized discounted cumulative gain@k for binary relevance\n",
    "  ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "  '''\n",
    "  batch_users = X_pred.shape[0]\n",
    "  idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "  topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                     idx_topk_part[:, :k]]\n",
    "  idx_part = np.argsort(-topk_part, axis=1)\n",
    "  \n",
    "  idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "  \n",
    "  tp = 1. / np.log2(np.arange(2, k+2))\n",
    "  \n",
    "  DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk].toarray() * tp).sum(axis=1)\n",
    "  IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                   for n in heldout_batch.getnnz(axis=1)])\n",
    "  return DCG / IDCG\n",
    "\n",
    "\n",
    "def recall(X_pred, heldout_batch, k=100):\n",
    "  batch_users = X_pred.shape[0]\n",
    "  \n",
    "  idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "  X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "  X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "  \n",
    "  X_true_binary = (heldout_batch > 0).toarray()\n",
    "  tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(np.float32)\n",
    "  \n",
    "  recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "  \n",
    "  return recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, device, data_in, data_out=None, shuffle=False, samples_perc_per_epoch=1):\n",
    "  assert 0 < samples_perc_per_epoch <= 1\n",
    "  \n",
    "  N = data_in.shape[0]\n",
    "  samples_per_epoch = int(N * samples_perc_per_epoch)\n",
    "  \n",
    "  if shuffle:\n",
    "    idx_list = np.arange(N)\n",
    "    np.random.shuffle(idx_list)\n",
    "    idx_list = idx_list[:samples_per_epoch]\n",
    "  else:\n",
    "    idx_list = np.arange(samples_per_epoch)\n",
    "  \n",
    "  for st_idx in range(0, samples_per_epoch, batch_size):\n",
    "    end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
    "    idx = idx_list[st_idx:end_idx]\n",
    "    \n",
    "    yield Batch(device, idx, data_in, data_out)\n",
    "    \n",
    "class Batch:\n",
    "  def __init__(self, device, idx, data_in, data_out=None):\n",
    "    self._device = device\n",
    "    self._idx = idx\n",
    "    self._data_in = data_in\n",
    "    self._data_out = data_out\n",
    "    \n",
    "  def get_idx(self):\n",
    "    return self._idx\n",
    "  \n",
    "  def get_idx_to_dev(self):\n",
    "    return torch.LongTensor(self.get_idx()).to(self._device)\n",
    "  \n",
    "  def get_ratings(self, is_out=False):\n",
    "    data = self._data_out if is_out else self._data_in\n",
    "    return data[self._idx]\n",
    "  \n",
    "  def get_ratings_to_dev(self, is_out=False):\n",
    "    return torch.Tensor(\n",
    "      self.get_ratings(is_out).toarray()\n",
    "    ).to(self._device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opts, train_data, batch_size, n_epochs, beta, gamma, dropout_rate):\n",
    "  # training mode\n",
    "  model.train()\n",
    "  \n",
    "  for epoch in range(n_epochs):\n",
    "    for batch in generate(batch_size, device, data_in=train_data, shuffle=True):\n",
    "      ratings = batch.get_ratings_to_dev()\n",
    "      \n",
    "      for optimizer in opts:\n",
    "        optimizer.zero_grad()\n",
    "      \n",
    "      _, loss = model(ratings, beta, gamma, dropout_rate)\n",
    "      loss.backward()\n",
    "      \n",
    "      for optimizer in opts:\n",
    "        optimizer.step()\n",
    "      \n",
    "    # for scheduler in sches:\n",
    "    #   scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_in, data_out, metrics, samples_perc_per_epoch=1, batch_size=500):\n",
    "  # evaluation mode\n",
    "  model.eval()\n",
    "  metrics = deepcopy(metrics)\n",
    "  \n",
    "  for m in metrics:\n",
    "    m['score'] = []\n",
    "    \n",
    "  for batch in generate(batch_size, \n",
    "                        device, \n",
    "                        data_in, \n",
    "                        data_out,\n",
    "                        samples_perc_per_epoch):\n",
    "    \n",
    "    items_in = batch.get_ratings_to_dev()\n",
    "    items_out = batch.get_ratings(is_out=True)\n",
    "    \n",
    "    items_pred = model(items_in, calculate_loss=False).cpu().detach().numpy()\n",
    "    \n",
    "    if not(data_in is data_out):\n",
    "      items_pred[batch.get_ratings().nonzero()] = -np.inf\n",
    "      \n",
    "    for m in metrics:\n",
    "      m['score'].append(m['metric'](items_pred, items_out, k=m['k']))\n",
    "      \n",
    "  \n",
    "  for m in metrics:\n",
    "    m['score'] = np.concatenate(m['score']).mean()\n",
    "    \n",
    "  return [x['score'] for x in metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "loader = DataLoader(args.data_dir)\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "n_users = loader.load_n_users()\n",
    "\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "  'hidden_dim': args.hidden_dim,\n",
    "  'latent_dim': args.latent_dim,\n",
    "  'input_dim': train_data.shape[1]\n",
    "}\n",
    "\n",
    "metrics = [{'metric': ndcg, 'k': 100}, {'metric': recall, 'k': 20}, {'metric': recall, 'k': 50}]\n",
    "\n",
    "best_ndcg = -np.inf\n",
    "train_scores, valid_scores = [], []\n",
    "\n",
    "model = RecVAE(**model_kwargs).to(device)\n",
    "model_best = RecVAE(**model_kwargs).to(device)\n",
    "\n",
    "learning_kwargs = {\n",
    "  'model': model,\n",
    "  'train_data': train_data,\n",
    "  'batch_size': args.batch_size,\n",
    "  'beta': args.beta,\n",
    "  'gamma': args.gamma\n",
    "}\n",
    "\n",
    "decoder_params = set(model.decoder.parameters())\n",
    "encoder_params = set(model.encoder.parameters())\n",
    "\n",
    "# https://pytorch.org/docs/stable/optim.html\n",
    "optimizer_encoder = optim.Adam(encoder_params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "optimizer_decoder = optim.Adam(decoder_params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "# scheduler_encoder = optim.lr_scheduler.CosineAnnealingLR(optimizer_encoder, T_max=50, eta_min=0.0001)\n",
    "# scheduler_decoder = optim.lr_scheduler.CosineAnnealingLR(optimizer_decoder, T_max=50, eta_min=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | valid ndcg@100: 0.1990 | best valid: 0.1990 | train ndcg@100: 0.3842\n",
      "epoch 1 | valid ndcg@100: 0.3140 | best valid: 0.3140 | train ndcg@100: 0.5074\n",
      "epoch 2 | valid ndcg@100: 0.3562 | best valid: 0.3562 | train ndcg@100: 0.5637\n",
      "epoch 3 | valid ndcg@100: 0.3815 | best valid: 0.3815 | train ndcg@100: 0.5983\n",
      "epoch 4 | valid ndcg@100: 0.3950 | best valid: 0.3950 | train ndcg@100: 0.6178\n",
      "epoch 5 | valid ndcg@100: 0.4050 | best valid: 0.4050 | train ndcg@100: 0.6313\n",
      "epoch 6 | valid ndcg@100: 0.4119 | best valid: 0.4119 | train ndcg@100: 0.6412\n",
      "epoch 7 | valid ndcg@100: 0.4174 | best valid: 0.4174 | train ndcg@100: 0.6494\n",
      "epoch 8 | valid ndcg@100: 0.4231 | best valid: 0.4231 | train ndcg@100: 0.6560\n",
      "epoch 9 | valid ndcg@100: 0.4269 | best valid: 0.4269 | train ndcg@100: 0.6608\n",
      "epoch 10 | valid ndcg@100: 0.4306 | best valid: 0.4306 | train ndcg@100: 0.6655\n",
      "epoch 11 | valid ndcg@100: 0.4331 | best valid: 0.4331 | train ndcg@100: 0.6692\n",
      "epoch 12 | valid ndcg@100: 0.4355 | best valid: 0.4355 | train ndcg@100: 0.6732\n",
      "epoch 13 | valid ndcg@100: 0.4383 | best valid: 0.4383 | train ndcg@100: 0.6765\n",
      "epoch 14 | valid ndcg@100: 0.4407 | best valid: 0.4407 | train ndcg@100: 0.6792\n",
      "epoch 15 | valid ndcg@100: 0.4429 | best valid: 0.4429 | train ndcg@100: 0.6814\n",
      "epoch 16 | valid ndcg@100: 0.4440 | best valid: 0.4440 | train ndcg@100: 0.6840\n",
      "epoch 17 | valid ndcg@100: 0.4454 | best valid: 0.4454 | train ndcg@100: 0.6866\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.n_epochs):\n",
    "  if args.not_alternating:\n",
    "    train(opts=[optimizer_encoder, optimizer_decoder], epochs=1, dropout_rate=args.dropout_rate, **learning_kwargs)\n",
    "  else:\n",
    "    train(opts=[optimizer_encoder], n_epochs=args.n_enc_epochs, dropout_rate=args.dropout_rate, **learning_kwargs)\n",
    "    model.update_prior()\n",
    "    train(opts=[optimizer_decoder], n_epochs=args.n_dec_epochs, dropout_rate=0, **learning_kwargs)\n",
    "  \n",
    "  train_scores.append(\n",
    "    evaluate(model, train_data, train_data, metrics, 0.01)[0]\n",
    "  )\n",
    "  \n",
    "  valid_scores.append(\n",
    "    evaluate(model, vad_data_tr, vad_data_te, metrics, 1)[0]\n",
    "  )\n",
    "  \n",
    "  if valid_scores[-1] > best_ndcg:\n",
    "    best_ndcg = valid_scores[-1]\n",
    "    model_best.load_state_dict(deepcopy(model.state_dict()))\n",
    "    with open(args.save, 'wb') as f:\n",
    "      torch.save(model, f)\n",
    "    \n",
    "  print(f'epoch {epoch} | valid ndcg@100: {valid_scores[-1]:.4f} | ' +\n",
    "        f'best valid: {best_ndcg:.4f} | train ndcg@100: {train_scores[-1]:.4f}')\n",
    "  \n",
    "  if (epoch % 10) == 0:\n",
    "    wandb.log({\"best valid\": best_ndcg, \"valid ndcg@100\": valid_scores[-1], \"train ndcg@100\": train_scores[-1]})\n",
    "\n",
    "# test data  \n",
    "test_metrics = [{'metric': ndcg, 'k': 100}, {'metric': recall, 'k': 20}, {'metric': recall, 'k': 50}]\n",
    "\n",
    "final_scores = evaluate(model_best, test_data_tr, test_data_te, test_metrics)\n",
    "\n",
    "for metric, score in zip(test_metrics, final_scores):\n",
    "  print(f\"{metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")\n",
    "\n",
    "wandb.log({\"test ndcg@100\": final_scores[0], \"test recall@20\": final_scores[1], \"test recall@50\": final_scores[2]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items: 6807\n",
      "# of users: 31360\n"
     ]
    }
   ],
   "source": [
    "# load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "  model = torch.load(f)\n",
    "  \n",
    "loader = DataLoader(args.data_dir)\n",
    "\n",
    "# load inference data\n",
    "inference_data = loader.load_data('inference')\n",
    "\n",
    "n_items = loader.load_n_items() # train_data.shape[1]\n",
    "n_users = loader.load_n_users() # train_data.shape[0]\n",
    "\n",
    "print(f'# of items: {n_items}')\n",
    "print(f'# of users: {n_users}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, data_in, samples_perc_per_epoch=1, batch_size=500):\n",
    "  model.eval()\n",
    "  output = []\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for batch in generate(batch_size, \n",
    "                          device,\n",
    "                          data_in,\n",
    "                          samples_perc_per_epoch):\n",
    "      \n",
    "      ratings_in = batch.get_ratings_to_dev()\n",
    "      \n",
    "      ratings_pred = model(ratings_in, calculate_loss=False).detach().cpu().numpy()\n",
    "      \n",
    "      # remove watched items\n",
    "      ratings_pred[batch.get_ratings().nonzero()] = -np.inf\n",
    "      \n",
    "      n_users = ratings_pred.shape[0]\n",
    "      for i in range(n_users):\n",
    "        output.append(ratings_pred[i])\n",
    "        \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_output = inference(model, inference_data)\n",
    "\n",
    "submission = []\n",
    "\n",
    "for idx in range(n_users):\n",
    "  # item descending order\n",
    "  sid_idx_preds_per_user = np.argsort(inference_output[idx])[::-1]\n",
    "  sid_preds = unique_sid[sid_idx_preds_per_user]\n",
    "  \n",
    "  tmp = []\n",
    "  for item in sid_preds:\n",
    "    if len(tmp) == 10:\n",
    "      break\n",
    "    tmp.append((unique_uid_before_shuffling[idx], item))\n",
    "  \n",
    "  submission.extend(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(submission, columns=['user', 'item'])\n",
    "submission_df.to_csv(f'./submissions/recvae_submission_exp{exp_idx}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>4370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>37386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>58025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>7373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>4886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user   item\n",
       "0    11   4370\n",
       "1    11  37386\n",
       "2    11  58025\n",
       "3    11   7373\n",
       "4    11   4886"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best valid</td><td>▁▇██████████████████</td></tr><tr><td>test ndcg@100</td><td>▁</td></tr><tr><td>test recall@20</td><td>▁</td></tr><tr><td>test recall@50</td><td>▁</td></tr><tr><td>train ndcg@100</td><td>▁▆▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>valid ndcg@100</td><td>▁▇██████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best valid</td><td>0.46919</td></tr><tr><td>test ndcg@100</td><td>0.47417</td></tr><tr><td>test recall@20</td><td>0.37695</td></tr><tr><td>test recall@50</td><td>0.43635</td></tr><tr><td>train ndcg@100</td><td>0.77196</td></tr><tr><td>valid ndcg@100</td><td>0.4673</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-leaf-73</strong> at: <a href='https://wandb.ai/new-recs/movierec/runs/56conbql' target=\"_blank\">https://wandb.ai/new-recs/movierec/runs/56conbql</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230611_162846-56conbql/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish WandB run\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
